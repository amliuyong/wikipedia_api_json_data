{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os, sys\n",
    "import json\n",
    "import datetime, time\n",
    "import urllib.parse\n",
    "import itertools\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "try:\n",
    "    import requests\n",
    "except ModuleNotFoundError:\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Missing modules. Run: pip install -r requirements.txt\")\n",
    "    print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "LOG_LEVEL = logging.INFO\n",
    "MAX_WORKERS = 10\n",
    "MAX_BATCH_SIZE = 50\n",
    "BULK_FILE_SIZE = 10485760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logging.basicConfig(format='%(asctime)s %(name)s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=LOG_LEVEL)\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "api_random = \"https://en.wikipedia.org/api/rest_v1/page/random/summary\"\n",
    "api_related = \"https://en.wikipedia.org/api/rest_v1/page/related/{}\"\n",
    "api_summary = \"https://en.wikipedia.org/api/rest_v1/page/summary/{}\"\n",
    "#API_URL_metadata = \"https://en.wikipedia.org/api/rest_v1/page/metadata/{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "data_path = \"data\"\n",
    "if not os.path.exists(data_path): os.makedirs(data_path)\n",
    "bulk_filename = (os.path.join(data_path,f\"wiki_{i}.bulk\") for i in itertools.count(1))\n",
    "data_filename = os.path.join(data_path,\"wiki.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate(size=1024,initial_titles=None):\n",
    "    _logger.info(f\"Starting with size {size} and initial title {initial_titles}\")\n",
    "    start = time.time()\n",
    "    data = list()\n",
    "    if not initial_titles: data.append(get_random())       \n",
    "    else: data.extend(get_summary(initial_titles))\n",
    "    processed_titles = set()\n",
    "    backlog = list()    \n",
    "    fdata = open(data_filename, \"w\")\n",
    "    fbulk = open(next(bulk_filename), \"w\")\n",
    "    try:\n",
    "        while True:\n",
    "            for doc in data:            \n",
    "                _title = doc[\"titles\"][\"canonical\"]\n",
    "                _pageid = doc[\"pageid\"]\n",
    "                if fdata.tell()>size: raise SizeReached\n",
    "                if _title in processed_titles: continue\n",
    "                doc[\"@timestamp\"] = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc).astimezone().replace(microsecond=0).isoformat() #iso timestamp\n",
    "                json.dump(doc,fdata)\n",
    "                fdata.write('\\n')\n",
    "                json.dump({ \"index\" : { \"_id\" : str(_pageid) } }, fbulk)\n",
    "                fbulk.write('\\n') \n",
    "                json.dump(doc, fbulk)\n",
    "                fbulk.write('\\n')\n",
    "                processed_titles.add(_title)\n",
    "                backlog.append(_title)\n",
    "            batch, backlog = backlog[:MAX_BATCH_SIZE], backlog[MAX_BATCH_SIZE:]\n",
    "            data_list = _run_parallel(MAX_WORKERS, get_related, batch)\n",
    "            data = list(itertools.chain.from_iterable(data_list))\n",
    "            if fbulk.tell()>=BULK_FILE_SIZE: fbulk = _increment_file_object(next(bulk_filename),previous=fbulk)                \n",
    "    except StopIteration:\n",
    "        print(\"Ran out of wikipedia related pages.\")\n",
    "    except SizeReached:\n",
    "        print(\"Reached the intended size.\")\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        print(f\"Final file size: {fdata.tell()} B, {fdata.tell()/float(1<<20)} MB\")\n",
    "        print(f\"Number of json docs: {len(processed_titles)}.\")\n",
    "        print(f\"Composed in: {(time.time()-start)/60} min\")    \n",
    "        fdata.close()\n",
    "        fbulk.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SizeReached(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_related(title):\n",
    "    with requests.get(api_related.format(urllib.parse.quote_plus(title))) as response:\n",
    "        response.raise_for_status()\n",
    "        _data = response.json()[\"pages\"]\n",
    "        return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_random():   \n",
    "    response = requests.get(api_random)\n",
    "    response.raise_for_status()\n",
    "    _item = response.json()\n",
    "    return _item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_summary(titles):  \n",
    "    _data = list()\n",
    "    for title in titles:\n",
    "        response = requests.get(api_summary.format(urllib.parse.quote_plus(title)))\n",
    "        response.raise_for_status()\n",
    "        _data.append(response.json())\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _run_parallel(max_workers, function, todo_list):\n",
    "    start = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as pool:\n",
    "        _data = list(pool.map(function,todo_list))\n",
    "    _logger.debug(f\"Completed function {function.__name__} on {len(todo_list)} items with {max_workers} workers in: {time.time()-start} s\")\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _increment_file_object(name, previous, mode=\"w\"):\n",
    "    _logger.info(f\"Rotating file {previous.name} to {name}\")\n",
    "    previous.close()\n",
    "    return open(name, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BULK_FILE_SIZE*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(size=(BULK_FILE_SIZE*10), initial_titles=[\"Amazon_(company)\", \"Google\", \"Facebook\", \"Microsoft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def main(**kwargs):\n",
    "    generate(size=kwargs[\"size\"], initial_titles=kwargs.get(\"titles\",None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Get JSON data from wikipedia REST API (summaries) related to specified initial titles up to defined size. Script also prepares chunked files for load to Elasticsearch via _bulk API.\",\n",
    "        epilog=f\"Example: python3 wikipedia_api_json_data.py 20971520 --titles Amazon_\\(company\\) Google Facebook Microsoft\")\n",
    "    parser.add_argument(\n",
    "        'size', help='Size of raw json data to get (in bytes).', type=int)\n",
    "    parser.add_argument(\n",
    "        '--titles', help='Initial titles to which get the related content (space separated). Make sure you specify valid titles.', nargs='+')\n",
    "    args = parser.parse_args()\n",
    "    return vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main(**_parse_args()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 exp/nb_wikipedia.py 104857600 --titles Amazon_(company) Google Facebook Microsoft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
